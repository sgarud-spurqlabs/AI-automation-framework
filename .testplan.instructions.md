---
name: test_plan_generation
description: Instructions for LLMs to generate an elaborate Test Plan (Markdown) based on requirements and generated test cases
applyTo: "requirements/*.md"
---

# Test Plan Generation — LLM Instructions (Senior QA)

Role: Act as a Senior QA Engineer. Use clear, professional language and produce a well-structured, document-quality Test Plan in Markdown format. Keep examples where helpful and keep the structure consistent for documentation.

Output rules
- Write the generated Test Plan to the repository directory `/Test-Output/` (create the directory if it does not exist).
- Filename pattern: `<module>-test-plan.md` where `<module>` is the feature/workflow name lowercased with spaces replaced by hyphens (e.g., `login-test-plan.md`, `add-employee-test-plan.md`).
- Format: Markdown (`.md`). Include a short front-matter header with `TestPlanID`, `Module`, `Author`, and `Date`.

Primary goal
- Produce an actionable Test Plan that a QA team can use to plan, execute, and report testing activities for the supplied requirement(s) and generated test cases. The Plan must reference acceptance criteria and map to the test cases produced in `/Test-Output/` where applicable.

Test Plan template (sections required)
1. Test Plan ID
   - Format suggestion: `ORAN-TP-<NNN>` (e.g., `ORAN-TP-001`). If previous plans exist in `/Test-Output/`, choose the next unused numeric suffix.
2. Project Name
   - e.g., "OrangeHRM Automation"
3. Module/Feature Overview
   - Short summary of the feature, user story, scope, and boundaries.
4. Test Plan Description
   - High-level description: what will be tested and what is out of scope.
5. Test Strategy
   - Approach: Manual vs Automation – which areas will be automated and why.
   - Tools: list tools and frameworks (e.g., Playwright (Java), TestNG, Cucumber, spreadsheets, test management).
   - Automation criteria: which test cases qualify for automation (smoke, regression, high-frequency, deterministic).
6. Test Objectives
   - Short bullet list describing verification goals mapped to acceptance criteria.
7. Test Deliverables
   - Items to be delivered: Test Plan (this doc), Test Cases (CSV files), Test Reports (HTML/JSON), Defect logs, Traceability matrix, Test data sets.
8. Testing Schedule / Milestones
   - Suggested timeline: Test design complete, Review, Execution (smoke, functional, regression), Report delivery, Sign-off.
9. Test Environment
   - Environments required (dev/stage/prod mirror), browser matrix, OS, network conditions, test data setup, required service accounts, and any external integrations.
10. Roles & Responsibilities
   - List stakeholders: QA Lead, Testers (manual), Automation Engineers, Developers, Product Owner; map responsibilities.
11. Risk & Mitigation
   - Identify key risks (e.g., flaky external API, missing test data, long-running exports) and mitigation plans.
12. Entry and Exit Criteria
   - Entry: code deployed to environment X, acceptance criteria written, test data available.
   - Exit: all P0/P1 test cases passed, open defects logged as per severity policy, test report created.
13. Test Case Design Approach
   - Describe techniques used (BVA, EP, Decision Table, State Transition, Usability checks). Map to the generated test-case CSV(s) and include selection criteria for automation.
14. Metrics / Reporting
   - Define metrics (test execution progress, pass/fail rate, defect density, test coverage of ACs). Include reporting cadence and output locations (e.g., `target/cucumber-reports/`, `/Test-Output/`).
15. Approvals
   - Signature or names for Test Plan approval (QA Lead, Product Owner, Dev Lead) and date.

Additional recommended sections (optional but encouraged)
- Preconditions and Test Data Setup: provide exact steps to provision test data, accounts, or seed databases.
- Traceability Matrix: small table mapping Acceptance Criteria -> TestPlan Section -> TestCase IDs (from `/Test-Output/*.csv`).
- Known Limitations & Assumptions: list what is assumed and what is not covered.
- Automation Roadmap: prioritized list of test cases recommended for automation with estimated effort.

Generation rules and guidelines for the LLM
- Input: use the user story/requirement text and the generated test-cases (if present) as primary sources. Also use any "impact details" or risk notes provided.
- Role-specific voice: write as a Senior QA engineer, concise and authoritative. Use bullet lists, tables where helpful, and short examples.
- Examples: include at least one short example for the Test Environment and Traceability Matrix sections.
- Mapping: include a short Traceability Matrix that maps acceptance criteria or story sections to the TestCase IDs found in `/Test-Output/<module>-test-cases.csv`. If no CSV exists, list the planned mapping or note that test cases are not yet generated.
- IDs and filenames: ensure `TestPlanID` and all referenced TestCase IDs are consistent and follow the naming conventions used elsewhere (`ORAN-TC-###` for test cases, `ORAN-TP-###` for test plans).
- Deterministic output: given the same inputs, produce the same Test Plan content (stable ordering, consistent headings).

Output delivery
- Write the Test Plan Markdown file to `/Test-Output/<module>-test-plan.md`.
- The first lines of the file must include a short front-matter block with `TestPlanID`, `Module`, `Author`, and `Date`.
- After writing the file, return a short confirmation message listing the created file path and the TestPlanID.

Safety & constraints
- Do not overwrite existing Test Plan files unless the user explicitly asks to update/replace them. If a file exists, choose the next available `TestPlanID` and write a new file, or ask the user whether to replace.
- Do not modify code or test-case CSVs; only create the new Test Plan `.md` under `/Test-Output/`.

Example minimal front-matter (top of generated file):

```
TestPlanID: ORAN-TP-001
Module: Login
Author: QA Senior
Date: 2025-12-12
```

If you'd like, I can now generate a Test Plan for the `Login` workflow (or `Add Employee`, or both). Reply with which module(s) to process, or say `all` to generate for both.
