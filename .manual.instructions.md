---
name: manual_test_case_generation
description: Instructions for LLMs to generate manual test cases in CSV for given requirements/workflows
applyTo: "requirements/*.md"
---

# Manual Test Case Generation — LLM Instructions

Purpose: Give an LLM a precise, repeatable set of rules so it can generate manual test cases for a given requirement or workflow and write them to a CSV file under `/test-cases/`.

When asked to "Generate test cases" follow these rules exactly:

- Scope: Generate test cases for the workflow(s) requested. Example workflows in this repo: `Login`, `Add Employee`.
- Output location: write a CSV file into the repository path `/test-cases/` (create the directory if missing).
- Filename: `<module>-test-cases.csv` where `<module>` is the workflow name lowercased with spaces replaced by hyphens (e.g., `login-test-cases.csv`, `add-employee-test-cases.csv`).
- Maximum coverage: create as many relevant and distinct test cases as reasonably possible for the workflow (positive, negative, boundary, validation, UI, accessibility where applicable).

CSV format (columns) — the LLM must produce a CSV with exactly these headers in this order:

- `TestId` — Start from `ORAN-TC-068` and increment by 1 for each new test case (ORAN-TC-068, ORAN-TC-069, ...).
- `TestName` — Short descriptive name (one line).
- `StepNumber` — Integer step index (1..N) for the step within the test case.
- `Action` — The user or tester action to perform for that step.
- `TestData` — Any data required for that step (leave blank if none); for multiple fields, use new lines or spaces, do not use commas.
- `ExpectedResult` — The expected outcome for that step. **Every step must have an ExpectedResult**. If a test case has multiple steps, each step must list its corresponding expected result.
- `Priority` — One of `P0`, `P1`, `P2`, `P3` (P0 highest). Default to `P1` unless the step is critical (use P0).
- `Severity` — One of `Critical`, `High`, `Medium`, `Low`.
- `Note` — Optional free-text note; may include preconditions, cleanup, or links to related stories.

IMPORTANT: Do not include the comma character (`,`) inside any column value. This CSV uses commas as column separators; embedding commas within fields will break the file structure and parsing. If you must include multiple sub-values in a field, use one of these alternatives instead of commas:
- Use semicolons (`;`) to separate sub-values.
- Use spaces or new lines to separate items.

Generation rules and patterns:

- Test case granularity: treat each CSV row as a single test *step* within a logical test case. To group steps into a single test case, repeat the same `TestId` and `TestName` across multiple rows with increasing `StepNumber` values.
- Test case boundaries: start a new `TestId` for each logical test case (not per step). Ensure `StepNumber` restarts at 1 for each `TestId`.
- Data sensitivity: never include real secrets. use anonymized placeholder values.
- Idempotency: generated files must be deterministic given the same prompt. Use the same starting `TestId` and deterministic ordering (happy path first, then negative cases, then edge cases).
- Validation checks: include both UI-level expectations (e.g., "Error message displayed: 'Invalid credentials'") and backend-level confirmations when apparent from the workflow (e.g., "New employee appears in Employee List").
- Soft-delete handling: if a delete action may be soft-delete, include a cleanup step (restore or verify recoverability) in a separate test case or a Note.


Senior QA engineer mode (advanced generation)

When the user requests "senior" or "expert" level test design, or provides existing test cases and impact details, follow these additional rules:

- Role: Act as a senior QA engineer. Use domain knowledge and testing heuristics to design high-quality test cases that cover functionality, edge-cases and business impact.
- Input awareness: Accept three inputs where available: the user story (requirement), the set of existing test cases (CSV or list), and an "impact details" note describing risk, affected modules, or regression scope.
- Avoid duplication: Compare new test ideas against the provided existing test cases and intentionally produce new, non-duplicate tests. If a test overlaps an existing one but extends it (different data, more steps, or additional assertions), mark it in `Note` as "derived-from: <TestId>".
- Test-design techniques: Apply multiple design techniques to maximize coverage and evidence completeness:
	- Boundary Value Analysis (BVA) for numeric/range inputs
	- Equivalent Partitioning (EP) for input domains
	- Decision Table testing for combinational logic or permission matrices
	- State Transition testing for multi-step flows or lifecycle states
	- Exploratory / negative cases focusing on error handling and validation
- Prioritization and mapping: Map each generated test case to acceptance criteria or impact items in the `Note` field (e.g., "maps-to: AC1,AC3; impact: login-authentication"). Use `Priority` and `Severity` to reflect business impact from the impact details.
- Naming and structure: Use consistent `TestName` prefixes: `<workflow> - <short-description>` (e.g., "Login - Empty username validation"). Keep names concise and unique.
- Decision artifacts: When you create decision-table or state-transition tests, add a short `Note` describing the decision table rows or states covered (one-line summary is fine).
- Traceability: If the user provided an existing test cases file, include a summary line at the top of the generated CSV `Note` rows indicating how many new cases were added and which existing TestIds were considered.

Example advanced requirement: If given a user story, existing tests and impact note, generate the new tests prioritized by risk and mark derived tests as described.

Example: For a Login enhancement with impact on authentication and session handling, include BVA (password length, username length), EP (valid/invalid char sets), decision-table for combinations (locked account + correct password), and state-transition for session timeout handling.

Example CSV `Note` entry when deriving a variant:
"derived-from: ORAN-TC-070; maps-to: AC2; decision-table-row: 3"

Example `Note` entry for traceability summary (use a single-row Note under the first TestId generated):
"derived-from-count: 2; existing-considered: [ORAN-TC-064, ORAN-TC-065]; new-cases: 5"

Example: For a simple Login test case (two steps) the CSV will contain two rows with the same `TestId`:

TestId,TestName,StepNumber,Action,TestData,ExpectedResult,Priority,Severity,Note
ORAN-TC-068,Successful login,1,Open login page,,Login page loads with username/password fields visible,P1,Medium,
ORAN-TC-068,Successful login,2,Enter valid credentials and click Login,{"username":"Admin","password":"admin123"},Dashboard is displayed and user is logged in,P0,Critical,

Operational instructions for the LLM agent (how to run):

1. Parse the user's request to identify the workflow/module name(s) (e.g., "Login", "Add Employee").
2. Generate test cases following the rules above. Start `TestId` at `ORAN-TC-068` or continue numbering if `test-cases/` already contains files generated earlier — in that case, choose the next unused integer suffix.
3. Write a UTF-8 CSV file to `/test-cases/<module>-test-cases.csv` using commas as separators and double quotes where fields contain commas or newlines.
4. Return a short confirmation message listing the created file path and the range of `TestId` values used.

Permissions & safety:

- Do not modify any existing source files except to add the output CSV under `/test-cases/`.
- If the requested workflow is ambiguous, ask a clarifying question before generating files.

If you want, I can now generate test cases for the `Login` and `Add Employee` workflows and add the CSV files to `/test-cases/`. Reply with which workflows to process (or say `all` to do both). 
